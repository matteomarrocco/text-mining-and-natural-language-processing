# -*- coding: utf-8 -*-
"""PROJECT TEXT MINING NEW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p9s4QB6_N8dOAVQFJUY-mK8R2-pT5Mqg

Matteo Marrocco, i6371541

#Import Modules and Data
"""

import os
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.probability import FreqDist
import re

from google.colab import drive
drive.mount('/content/drive')

import os

directory = [
    "/content/drive/MyDrive/Murder on the Orient Express.txt",
    "/content/drive/MyDrive/Death on the Nile.txt",
    "/content/drive/MyDrive/The Murder of Roger Ackroyd.txt"
]

books = []

for file_path in directory:
    if file_path.endswith(".txt"):
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        books.append(text)

book1, book2, book3 = books

"""#Preprocessing

This is fundamental, in order to clean the data and pass them for doing NER and Sentiment Analysis
"""

import nltk
nltk.download('punkt')

text=book1

raw_text = book1
start_index = raw_text.find('It was five o\'clock')
text = raw_text[start_index:]

"""Let's start preprocessing"""

def remove_header(text):
    start_phrase = ""
    start_index = text.find(start_phrase)
    if start_index != -1:
        content = text[start_index + len(start_phrase):]
        return content.strip()
    else:
        return text.strip()
text=remove_header(text)
text[:1000]

"""##Create a function to remove stopwords and numbers"""

nltk.download('stopwords')
from nltk.corpus import stopwords

def prep(text, remove_numbers=True, remove_stopwords=True):
    text = text.replace('\n', ' ')
    text = re.sub(' +', ' ', text)
    text = text.replace('\r', '')

    if remove_numbers:
        text = re.sub(r'\d+', '', text)

    if remove_stopwords:
        stopword_list = set(stopwords.words('english'))
        tokens = text.split()
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        text = ' '.join(filtered_tokens)

    return text

text = prep(text, remove_numbers=False, remove_stopwords=True)

"""#don't remove stopwords"""

nltk.download('stopwords')
from nltk.corpus import stopwords

def prep(text, remove_numbers=True, remove_stopwords=True):
    text = text.replace('\n', ' ')
    text = re.sub(' +', ' ', text)
    text = text.replace('\r', '')

    if remove_numbers:
        text = re.sub(r'\d+', '', text)

    if remove_stopwords:
        stopword_list = set(stopwords.words('english'))
        tokens = text.split()
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
        text = ' '.join(filtered_tokens)

    return text

text = prep(text, remove_numbers=False, remove_stopwords=False)

text[:1000]

"""##Tokenizing, removing punctuations, strange signs, and peforming lemmatization"""

import re

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tokenize import RegexpTokenizer

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer=WordNetLemmatizer()
sentences = sent_tokenize(text)
tokenizer=RegexpTokenizer(r"[\w']+")
tokens = []

for sentence in sentences:
    sentence_tokens = tokenizer.tokenize(sentence)
    sentence_tokens = [token.lower() for token in tokenizer.tokenize(sentence)]
    sentence_tokens = [re.sub(r'\W', '', token) for token in sentence_tokens]  # Remove special characters
    tokens.extend(sentence_tokens)

tagged_sentences = nltk.pos_tag_sents([tokenizer.tokenize(sentence) for sentence in sentences])

lemmatized_tokens = []
for tagged_sentence in tagged_sentences:
    for token, tag in tagged_sentence:
        if tag.startswith('N'):
            lemma = lemmatizer.lemmatize(token, pos='n')
        elif tag.startswith('V'):
            lemma = lemmatizer.lemmatize(token, pos='v')
        elif tag.startswith('J'):
            lemma = lemmatizer.lemmatize(token, pos='a')
        elif tag.startswith('R'):
            lemma = lemmatizer.lemmatize(token, pos='r')
        else:
            lemma = token
        lemmatized_tokens.append(lemma)

print("Lemmatized Tokens", lemmatized_tokens)

"""#Preprocessing Results

##Sentences Split
"""

sentences[:5]

"""##Tokens split"""

tokens[:10]

"""##Tokens Split and lemmatized"""

lemmatized_tokens[:20]

"""#Sentences split and lemmatized"""

lemmatized_tokens_set = set(lemmatized_tokens)
filtered_sentences = []

for sentence in sentences:
    words = word_tokenize(sentence)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

    if any(word in lemmatized_tokens_set for word in lemmatized_words):
        filtered_sentences.append(sentence)

filtered_sentences[:10]

"""#Visualizations after Preprocessing"""

#longtail of words
freq_dist = nltk.FreqDist(tokens)
print(freq_dist)
print(freq_dist.most_common(50))
freq_dist.plot(500)

"""When doing wordclouds, I observe that most of the most frequent words dont provide any meaningful information for the topics of the book.

"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt
for token in lemmatized_tokens:
  lemma_text = ' '.join(lemmatized_tokens)

print(lemma_text[:100])

wordcloud = WordCloud(width=800, height=400).generate(lemma_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""###Bigrams

Let's visualize also bigrams
"""

from nltk import bigrams
bigram_tokens = list(bigrams(lemmatized_tokens))

bigram_text = ' '.join(['_'.join(bigram) for bigram in bigram_tokens]) # I Create a string of bigrams

wordcloud = WordCloud(width=800, height=400).generate(bigram_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""##Trigrams

Neither trigrams seem very useful
"""

from nltk import trigrams

trigram_tokens = list(trigrams(lemmatized_tokens))

trigram_text = ' '.join(['_'.join(trigram) for trigram in trigram_tokens])

wordcloud = WordCloud(width=800, height=400).generate(trigram_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""#TF-SCORES

Is another technique for finding meaningful words based on their frequency in the text and rarity in the corpus

I use this technique studying the filtered words, without taking in consideration the stop words.

###Counting general words frequencies
"""

tf_scores = nltk.FreqDist(lemmatized_tokens)
most_frequent_words = tf_scores.most_common(500)
for word, count in most_frequent_words:
    print(f"Word: {word}, Count: {count}")

"""##TF-IDF

TF-IDF to find meaningful words based on their frequency in the text and rarity in the corpus


TF-IDF by treating each sentence as a collection of documents:

TF-IDF not considering sentences as documents
"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

tfidf_matrix = vectorizer.fit_transform([text])
feature_names = vectorizer.get_feature_names_out()

tfidf_values = tfidf_matrix.toarray()[0]
word_tfidf = dict(zip(feature_names, tfidf_values))
sorted_words = sorted(word_tfidf.items(), key=lambda x: x[1], reverse=True)

top_words = sorted_words[:40]
for word, tfidf in top_words:
    print(f"{word}: {tfidf}")

"""##TF-IDF using bigrams

Let's see the difference when using bigrams
"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(ngram_range=(2, 2))

tfidf_matrix = vectorizer.fit_transform([text])

feature_names = vectorizer.get_feature_names_out()

tfidf_values = tfidf_matrix.toarray()[0]

bigram_tfidf = dict(zip(feature_names, tfidf_values))

sorted_bigrams = sorted(bigram_tfidf.items(), key=lambda x: x[1], reverse=True)

top_bigrams = sorted_bigrams[:30]
for bigram, tfidf in top_bigrams:
    print(f"{bigram}: {tfidf}")

"""#Dependency Parsing

I have the aim to "study" the dependencies in order to further apply NER in a more correct way.
"""

!pip install stanza

!pip install tqdm

"""##Applying Dependency Parsing

I have to apply it on the "sentences" or "word" preprocessed in order to have a more cleaned and better data.

The process takes around 20 minutes.
"""

import stanza
import nltk
import pickle
from tqdm import tqdm

nltk.download('punkt')

stanza.download('en')

nlp = stanza.Pipeline('en')

sentences=filtered_sentences

try:
    with open('parsed_sentences.pkl', 'rb') as f:
        parsed_sentences = pickle.load(f)
except FileNotFoundError:
    parsed_sentences = []
    for sentence in tqdm(sentences, desc='Processing sentences'):
        doc = nlp(sentence)
        dependency_relations = [(word.text, word.head, word.deprel) for sent in doc.sentences for word in sent.words]
        parsed_sentences.append((sentence, dependency_relations))

    with open('parsed_sentences.pkl', 'wb') as f:
        pickle.dump(parsed_sentences, f)

for sentence, relations in parsed_sentences:
    print(f"Sentence: {sentence}")
    for relation in relations:
        word, head, deprel = relation
        print(f"{word} --{deprel}--> {head}")
    print()

"""#Correference Resolution: SOLUTION 1

Another important task, before applying NER is to find and study how words "him", "his",... are related to which person.

This is important because in this way, we can apply a better NER
"""

!pip install fastcoref

from fastcoref import FCoref

model = FCoref(device='cuda:0')

"""##Applying Coreference Resolution

I need to apply it to the sentences that I've done dependency parsing
"""

import pickle

file_path = "/content/drive/My Drive/resolved_sentences.pkl"

try:
    with open(file_path, 'rb') as f:
        resolved_sentences = pickle.load(f)
except FileNotFoundError:
    resolved_sentences = []
    for parsed_sentence in parsed_sentences:
        sentence_text = parsed_sentence[0]
        coref_result = model.predict(sentence_text)
        resolved_sentences.append(coref_result)

    with open('resolved_sentences.pkl', 'wb') as f:
        pickle.dump(resolved_sentences, f)

print(resolved_sentences)

"""#Coreference Resolution: SOLUTION 2

This solutioin has been given by the TA; so I should use it
"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from typing import List, Tuple
from nltk.tokenize.treebank import TreebankWordDetokenizer

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def get_span_noun_indices(doc, cluster):
    spans = [doc[span[0]:span[1]] for span in cluster]
    spans_pos = [pos_tag(word_tokenize(span)) for span in spans]
    span_noun_indices = [
        i for i, span_pos in enumerate(spans_pos) if any(pos in span_pos[0] for pos in ["NN", "NNS", "NNP", "NNPS"])
    ]
    return span_noun_indices

def get_cluster_head(doc, cluster, noun_indices):
    head_idx = noun_indices[0]
    head_start, head_end = cluster[head_idx]
    head_span = doc[head_start:head_end]
    return head_span, [head_start, head_end]

def core_logic_part(document: str, coref: List[int], resolved: List[str], mention_span: str):
    return document[:coref[0]] + mention_span + document[coref[1]:]

def span_to_index(span, token_spans):
  return [i for i, token_span in enumerate(token_spans) if (token_span[0] >= span[0] and token_span[1]<=span[1])]

def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):
        return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])

def resolve_coref(doc: str, resolve_text=False) -> str:
    preds = model.predict(texts=[doc])
    clusters = preds[0].get_clusters(as_strings=False)
    tokens = word_tokenize(doc)
    token_spans = nltk.tokenize.util.align_tokens(tokens, doc)
    if resolve_text:
        resolved = [tok+" " for tok in tokens]
        #resolved = [tok for tok in tokens]
        all_spans = [span for cluster in clusters for span in cluster]
        for cluster in clusters:
            indices = get_span_noun_indices(doc, cluster)
            if indices:
                mention, mention_span = get_cluster_head(doc, cluster, indices)
                for coref in cluster:
                    if list(coref) != mention_span and not is_containing_other_spans(coref, all_spans):
                        span_indices = span_to_index(coref, token_spans)
                        resolved[span_indices[0]] = mention + " "
                        for i in range(span_indices[0], span_indices[-1]):
                          resolved[i+1] = ""
        resolved_text = "".join([item for item in resolved if item!=" "])
    else:
        resolved_text = doc
    return resolved_text, clusters

"""##Applying Coreference Resolution"""

!pip install fastcoref

from fastcoref import FCoref


model = FCoref()

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from nltk.tokenize.treebank import TreebankWordDetokenizer
from typing import List, Tuple

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

chunks = [filtered_sentences[i:i + 100] for i in range(0, len(filtered_sentences), 100)]

resolved_texts = []

for chunk in chunks:
    document = ' '.join(chunk)

    resolved_sentences = []
    for sentence in sent_tokenize(document):
        original_sentence = sentence
        sentence = sentence.replace('"', '``')
        try:
            resolved_text, _ = resolve_coref(sentence, resolve_text=True)
            resolved_sentences.append(resolved_text)
        except IndexError:
            print(f"Error processing sentence: {sentence}")
            resolved_sentences.append(original_sentence)
        sentence = original_sentence

    resolved_texts.extend(resolved_sentences)

final_resolved_text = ' '.join(resolved_texts)

print("Resolved Text:", final_resolved_text)

"""I can try in this way:"""

import numpy as np
from nltk.tokenize import sent_tokenize

filtered_sentences = np.array(filtered_sentences[:100])

chunk_size = 10

chunks = np.array_split(filtered_sentences, len(filtered_sentences) // chunk_size)

def process_chunk(chunk):
    document = ' '.join(chunk)
    resolved_sentences = []
    for sentence in sent_tokenize(document):
        original_sentence = sentence
        sentence = sentence.replace('"', '``')
        try:
            resolved_text, _ = resolve_coref(sentence, resolve_text=True)
            resolved_sentences.append(resolved_text)
        except IndexError:
            print(f"Error processing sentence: {sentence}")
            resolved_sentences.append(original_sentence)
        sentence = original_sentence
    return resolved_sentences

resolved_texts = [sentence for chunk in resolved_texts for sentence in chunk]

final_resolved_text = ' '.join(resolved_texts)

with open('resolved_text.txt', 'w') as f:
    f.write(final_resolved_text)

test_sentence = "This is a test sentence."
resolved_text, _ = resolve_coref(test_sentence, resolve_text=True)
print(resolved_text)

import numpy as np
import re
from nltk.tokenize import sent_tokenize

filtered_sentences = np.array(filtered_sentences[:20])
chunk_size = 10
chunks = np.array_split(filtered_sentences, len(filtered_sentences) // chunk_size)

def replace_pronouns_with_nouns(sentence, clusters):
    for cluster in clusters:
        print(f"Cluster: {cluster}")
        noun = cluster[0]
        for word in cluster[1:]:
            if isinstance(word, str):
                sentence = re.sub(r'\b' + re.escape(word) + r'\b', ' ' + noun + ' ', sentence)
    return sentence


def process_chunk(chunk):
    document = ' '.join(chunk)
    resolved_sentences = []
    for sentence in sent_tokenize(document):
        original_sentence = sentence
        sentence = sentence.replace('"', '``')
        try:
            resolved_text, clusters = resolve_coref(sentence, resolve_text=True)
            resolved_text = replace_pronouns_with_nouns(resolved_text, clusters)
            resolved_sentences.append(resolved_text + ' ')
        except IndexError:
            print(f"Error processing sentence: {sentence}")
            resolved_sentences.append(original_sentence + ' ')
        sentence = original_sentence
    return resolved_sentences

resolved_texts = [sentence + ' ' for chunk in chunks for sentence in process_chunk(chunk)]

final_resolved_text = ''.join(resolved_texts)

with open('resolved_text.txt', 'w') as f:
    f.write(final_resolved_text)

print("Resolved Text:", final_resolved_text)



"""# Named Entity Recognition (NER) with Nltk

Fundamental for extracting names and entities in the text, that I've cleaned, preprocessed, depend parsed, and corref resoluted

##NER following the tutorial
"""

import numpy as np
import pandas as pd
import os
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk import word_tokenize, pos_tag, sent_tokenize

sent = pos_tag(word_tokenize(text))

!pip install svgling

from nltk import RegexpParser
from nltk.draw.tree import TreeView
from IPython.display import Image
import svgling

patterns= """mychunk:{<NN.*>+}"""
chunker = RegexpParser(patterns)
output = chunker.parse(sent)

from nltk.chunk import conlltags2tree, tree2conlltags
from pprint import pprint

iob_tagged = tree2conlltags(output)

nltk.download('maxent_ne_chunker')
nltk.download('words')
from nltk.chunk import ne_chunk

def extract_ne(trees, labels):

    ne_list = []
    for tree in ne_res:
        if hasattr(tree, 'label'):
            if tree.label() in labels:
                ne_list.append(tree)

    return ne_list

ne_res = ne_chunk(pos_tag(word_tokenize(text)))
labels = ['ORGANIZATION']

print(extract_ne(ne_res, labels))

sentences = sent_tokenize(text)

token_sentences = [word_tokenize(sent) for sent in sentences]
pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]
chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)

for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, "label"):
            print(chunk)

chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=False)

from pprint import pprint
import matplotlib.pyplot as plt
from collections import defaultdict, Counter

person_counts = Counter()
ner_categories = defaultdict(int)

for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, 'label'):
            ner_categories[chunk.label()] += 1
            if chunk.label() == 'PERSON':
                person_counts[chunk[0]] += 1

labels = list(ner_categories.keys())

values = [ner_categories.get(l) for l in labels]

fig = plt.figure(figsize=(8, 8))
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
plt.show()

top_20_persons = person_counts.most_common(20)

for person, count in top_20_persons:
    print(f'{person}: {count}')

!pip install stanza

!pip install transformers
!pip install peft

import stanza
import torch

nlp = stanza.Pipeline(lang='en', processors='tokenize,coref')

paragraphs = text.split('\n')

for paragraph in paragraphs:
    try:
        doc = nlp(paragraph)

        chains = doc.coref_chains

        for chain in chains:
            representative = chain.representative
            for mention in chain.mention:
                paragraph = paragraph.replace(mention, representative)
    except RuntimeError as e:
        if 'out of memory' in str(e):
            print(f" GPU ran out of memory: {paragraph}")
            torch.cuda.empty_cache()
        else:
            raise e



"""##NER: SOLUTION 1 with Nltk

"""

!pip install fastcoref

from fastcoref import FCoref

model = FCoref()

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from typing import List, Tuple
from nltk.tokenize.treebank import TreebankWordDetokenizer
from collections import defaultdict
from nltk.probability import FreqDist
import pickle
import torch
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

persons = []
locations = []
facilities=[]
labels=set()
ner_categories = defaultdict(int)

for sentence in filtered_sentences:
    if isinstance(sentence, str):
        words = word_tokenize(sentence)
        tagged_words = pos_tag(words)
        named_entities = ne_chunk(tagged_words)

        for entity in named_entities:
            if hasattr(entity, 'label'):
                ner_categories[entity.label()] += 1
                if entity.label() == 'PERSON':
                    person = ' '.join(c[0] for c in entity)
                    persons.append(person)
                elif entity.label() == 'GPE':
                    location = ' '.join(c[0] for c in entity)
                    locations.append(location)
                elif entity.label() == 'FACILITY':
                    facility = ' '.join(c[0] for c in entity)
                    facilities.append(facility)
                labels.add(entity.label())

freq_dist = FreqDist(persons)
location_freq_dist = FreqDist(locations)
fac_freq=FreqDist(facilities)

print("Most frequent persons:")
for person, frequency in freq_dist.most_common(15):
    print(person, frequency)

print("Most frequent locations:")
for person, frequency in location_freq_dist.most_common(10):
    print(person, frequency)

"""Some results are confusing and not perfect, so I need to adress this problem."""

import matplotlib.pyplot as plt

labels = list(ner_categories.keys())
sizes = list(ner_categories.values())

plt.figure(figsize=(10, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Named Entity Recognition Categories')
plt.show()

"""##Adjusting the results 1"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from typing import List, Tuple
from nltk.tokenize.treebank import TreebankWordDetokenizer
from collections import defaultdict
from nltk.probability import FreqDist
import pickle
import torch
import numpy as np
import re

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

persons = []
locations = []
facilities=[]
labels=set()
ner_categories = defaultdict(int)

for sentence in filtered_sentences:
    if isinstance(sentence, str):
        words = word_tokenize(sentence)
        tagged_words = pos_tag(words)
        named_entities = ne_chunk(tagged_words)

        for entity in named_entities:
            if hasattr(entity, 'label'):
                ner_categories[entity.label()] += 1
                if entity.label() == 'PERSON':
                    person = ' '.join(c[0] for c in entity)
                    persons.append(person)
                elif entity.label() == 'GPE':
                    location = ' '.join(c[0] for c in entity)
                    locations.append(location)
                elif entity.label() == 'FACILITY':
                    facility = ' '.join(c[0] for c in entity)
                    facilities.append(facility)
                labels.add(entity.label())

freq_dist = FreqDist(persons)
location_freq_dist = FreqDist(locations)
fac_freq=FreqDist(facilities)

filtered_sentences = np.array(filtered_sentences)

chunk_size = 10
chunks = np.array_split(filtered_sentences, len(filtered_sentences) // chunk_size)

def replace_pronouns_with_nouns(sentence, clusters):
    for cluster in clusters:
        noun = cluster[0]
        for word in cluster[1:]:
            if isinstance(word, str):
                sentence = re.sub(r'\b' + re.escape(word) + r'\b', ' ' + noun + ' ', sentence)
    return sentence

def process_chunk(chunk):
    document = ' '.join(chunk)
    resolved_sentences = []
    for sentence in sent_tokenize(document):
        original_sentence = sentence
        sentence = sentence.replace('"', '``')
        try:
            resolved_text, clusters = resolve_coref(sentence, resolve_text=True)
            resolved_text = replace_pronouns_with_nouns(resolved_text, clusters)
            resolved_sentences.append(resolved_text + ' ')
        except IndexError:
            print(f"Error processing sentence: {sentence}")
            resolved_sentences.append(original_sentence + ' ')
        sentence = original_sentence
    return resolved_sentences

resolved_texts = [sentence + ' ' for chunk in chunks for sentence in process_chunk(chunk)]
final_resolved_text = ''.join(resolved_texts)

with open('resolved_text.txt', 'w') as f:
    f.write(final_resolved_text)

print("Resolved Text:", final_resolved_text)

entity_synonyms = {
    "Poirot": ["Poirot", "M. Poirot", "Hercule Poirot", "M. Hercule Poirot", "Poiret", "POIROT", "Hercule Poirot —", "Monsieur Poirot", "##IROT","Po","HERCULE"],
    "M. Bouc": ["M. Bouc", "M. Bouc.", "Monsieur Bouc"],
    "Ratchett": ["Ratchett", "M. Ratchett", "Samuel Edward Ratchett", "Ratchett —", "Samuel Edward Ratchett —"],
    "Hubbard": ["Hubbard", "Mrs Hubbard", "Caroline Martha Hubbard"],
    "Arbuthnot": ["Arbuthnot"],
    "MacQueen": ["MacQueen", "M. MacQueen", "Hector MacQueen", "Hector Willard MacQueen"],
    "Armstrong": ["Armstrong", "Daisy Armstrong", "Sonia Armstrong", "Tommy Armstrong", "Selby Armstrong", "Toby Armstrong", "Armstrongs", "John Armstrong"],
    "Hardman": ["Hardman", "M. Hardman", "Cyrus Bethman Hardman", "Cyrus B. Hardman McNeil", "Cyrus Hardman", "Mr Hardman"],
    "Monsieur le Comte": ["Monsieur", "M. le", "Monsieur le Comte"],
    "Debenham": ["Debenham", "Mary Debenham", "Mary Hermione Debenham", "DEBEN", "Miss Debenham", "Mary —"],
    "Andrenyi": ["Andrenyi", "Elena Andrenyi", "Helena Andrenyi"],
    "Dragomiroff": ["Dragomiroff", "Madame la Princesse Dragomiroff", "Natalia Dragomiroff","Natalia","Madame la Princesse"],
    "Schmidt": ["Hildegarde Schmidt", "Fraulein Schmidt", "Schmidt"],
    "Michel": ["Michel", "Pierre Michel", "Michel —"],
    "Cassetti": ["Cassetti", "Cassetti —", "Cass"],
    "Arden": ["Linda Arden"],
    "Helena": ["Helena", "Elena", "Helena Goldenberg", "Elena Maria"],
    "Ohlsson": ["Greta Ohlsson", "Great Ohlsson", "Miss Ohlsson"],
    "Foscarelli": ["Antonio Foscarelli", "Foscarelli"],
    "Masterman": ["Masterman", "Edward Masterman", "Edward Henry Masterman"],
    "Harris": ["Harris", "M. Harris"],
    "Goldenberg": ["Helena Goldenberg", "Goldenberg"],
    "Daisy": ["Daisy", "Daisy —"],
    "Sonia": ["Sonia", "Sonia Armstrong"],
    "Miss Freebody": ["Miss Freebody", "Freebody"]
}

normalized_entities = {synonym: key for key, synonyms in entity_synonyms.items() for synonym in synonyms}

normalized_entities_list = [normalized_entities[entity] if entity in normalized_entities else entity for entity in persons]

freq_dist_normalized = FreqDist(normalized_entities_list)

top_10_characters = freq_dist_normalized.most_common(10)

for character, frequency in top_10_characters:
    print(f"{character}: {frequency}")



"""Now, I try in another way:"""

!pip install fastcoref

from fastcoref import FCoref

model = FCoref(enable_progress_bar=False)

resolved_sentences = []
for sentence in sentences[:5]:
    resolved_sentence = model.predict(sentence)
    resolved_sentences.append(resolved_sentence)

resolved_texts = [result.text for result in resolved_sentences]

resolved_text = ''.join(resolved_texts)
resolved_text

entity_synonyms = {
    "Poirot": ["Poirot", "Hercule Poirot", "Hercule", "Monsieur Poirot", "Discreetly M. Poirot", "Poirot Ratchett MacQueen", "Poirot Miss Debenham"],
    "Ratchett": ["Ratchett", "Samuel Edward Ratchett", "Ratchett Cassetti", "Ratchett MacQueen", "Hector Ratchett", "Samuel Edward"],
    "Bouc": ["Bouc", "Monsieur Bouc", "M. Bouc", "Poirot M. Bouc"],
    "Hardman": ["Hardman", "Cyrus Bethman Hardman", "Cyrus B. Hardman McNeil", "Monsieur Hardman"],
    "Hubbard": ["Hubbard", "Mrs Hubbard", "Martha Hubbard", "Mrs Arabella Richardson"],
    "Wagon Lit": ["Wagon Lit", "Wagon", "Wagons Lits"],
    "Armstrong": ["Armstrong", "Daisy Armstrong", "Sonia Armstrong", "Mrs Armstrong"],
    "Debenham": ["Miss Debenham", "Mary Debenham", "Miss", "Debenham", "Mary", "Miss Mary Debenham", "Mary Hermione Debenham", "Miss Debenham Colonel Arbuthnot"],
    "Arbuthnot": ["Arbuthnot", "Colonel Arbuthnot", "Arbuthnot MacQueen"],
    "Michel": ["Michel", "Pierre Michel", "Pierre"],
    "Count Andrenyi": ["Count", "Count Andrenyi", "Count Countess Andrenyi", "Count Countess", "Count Andrenyi Hector"],
    "Schmidt": ["Hildegarde Schmidt", "Schmidt", "Fraulein Schmidt", "Hildegarde", "Hildegarde Schmidt Greta Ohlsson"],
    "Dragomiroff": ["Princess Dragomiroff", "Princess"],
    "Ohlsson": ["Miss Ohlsson", "Greta Ohlsson"],
    "Constantine": ["Constantine", "Dr. Constantine"],
    "MacQueen": ["Hector MacQueen", "MacQueen"],
    "Mrs Armstrong": ["Mrs Armstrong"],
    "Antonio Foscarelli": ["Antonio", "Foscarelli", "Antonio Foscarelli"],
    "Miss Freebody": ["Miss Freebody"],
    "Helena Andrenyi": ["Helena", "Helena Elena", "Helena Goldenberg", "Helena Andrenyi"],
    "Elena": ["Elena", "Elena Maria", "Elena Andrenyi"],
    "Arden": ["Linda Arden", "Forest Arden Rosalind", "Arden"],
    "Samuel Edward Ratchett": ["Samuel Edward Ratchett", "Samuel Edward"],
    "Mary Debenham": ["Mary Debenham"]
}

normalized_entities = {synonym: key for key, synonyms in entity_synonyms.items() for synonym in synonyms}

import nltk
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from collections import defaultdict
from nltk.probability import FreqDist
import torch

nltk.download('averaged_perceptron_tagger')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def perform_ner(tagged_words: List[Tuple[str, str]]) -> List[Tuple[str, str]]:
    return ne_chunk(tagged_words)

def extract_entities(named_entities) -> Tuple[List[str], List[str], List[str]]:
    persons, locations, facilities = [], [], []
    for entity in named_entities:
        if hasattr(entity, 'label'):
            if entity.label() == 'PERSON':
                persons.append(' '.join(c[0] for c in entity))
            elif entity.label() == 'GPE':
                locations.append(' '.join(c[0] for c in entity))
            elif entity.label() == 'FACILITY':
                facilities.append(' '.join(c[0] for c in entity))
    return persons, locations, facilities

def normalize_entities(entity_synonyms: dict) -> dict:
    return {synonym: key for key, synonyms in entity_synonyms.items() for synonym in synonyms}

def resolve_sentences(model, sentences: List[str]) -> str:
    resolved_sentences = [model.predict(sentence).text for sentence in sentences]
    return ' '.join(resolved_sentences)


filtered_sentences=filtered_sentences


tagged_words = [pos_tag(word_tokenize(sentence)) for sentence in filtered_sentences]
named_entities = [perform_ner(words) for words in tagged_words]

entities = [extract_entities(entities) for entities in named_entities]

normalized_entities = normalize_entities(entity_synonyms)
resolved_text = resolve_sentences(model, filtered_sentences)

from nltk.probability import FreqDist


all_persons = [person for entity in entities for person in entity[0]]
all_locations = [location for entity in entities for location in entity[1]]

fdist_persons = FreqDist(all_persons)
fdist_locations = FreqDist(all_locations)

print("Most frequent persons:")
for person, frequency in fdist_persons.most_common(10):
    print(f"{person}: {frequency}")

print("\nMost frequent locations:")
for location, frequency in fdist_locations.most_common(10):
    print(f"{location}: {frequency}")

"""##Results used

"""

from collections import defaultdict
from nltk import pos_tag, ne_chunk
from nltk.tokenize import word_tokenize
from nltk.tree import Tree
import matplotlib.pyplot as plt
from nltk.probability import FreqDist

def get_continuous_chunks(text):
    chunked = ne_chunk(pos_tag(word_tokenize(text)))
    prev = None
    continuous_chunk = []
    current_chunk = []

    for i in chunked:
        if type(i) == Tree:
            current_chunk.append(" ".join([token for token, pos in i.leaves()]))
        elif current_chunk:
            named_entity = " ".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk

entity_synonyms = {
    "Poirot": ["Poirot", "Hercule Poirot", "Hercule", "Monsieur Poirot", "Discreetly M. Poirot", "Poirot Ratchett MacQueen", "Poirot Miss Debenham"],
    "Ratchett": ["Ratchett", "Samuel Edward Ratchett", "Ratchett Cassetti", "Ratchett MacQueen", "Hector Ratchett", "Samuel Edward"],
    "Bouc": ["Bouc", "Monsieur Bouc", "M. Bouc", "Poirot M. Bouc"],
    "Hardman": ["Hardman", "Cyrus Bethman Hardman", "Cyrus B. Hardman McNeil", "Monsieur Hardman"],
    "Hubbard": ["Hubbard", "Mrs Hubbard", "Martha Hubbard", "Mrs Arabella Richardson"],
    "Wagon Lit": ["Wagon Lit", "Wagon", "Wagons Lits"],
    "Armstrong": ["Armstrong", "Daisy Armstrong", "Sonia Armstrong", "Mrs Armstrong"],
    "Debenham": ["Miss Debenham", "Mary Debenham", "Miss", "Debenham", "Mary", "Miss Mary Debenham", "Mary Hermione Debenham", "Miss Debenham Colonel Arbuthnot"],
    "Arbuthnot": ["Arbuthnot", "Colonel Arbuthnot", "Arbuthnot MacQueen"],
    "Michel": ["Michel", "Pierre Michel", "Pierre"],
    "Count Andrenyi": ["Count", "Count Andrenyi", "Count Countess Andrenyi", "Count Countess", "Count Andrenyi Hector"],
    "Schmidt": ["Hildegarde Schmidt", "Schmidt", "Fraulein Schmidt", "Hildegarde", "Hildegarde Schmidt Greta Ohlsson"],
    "Dragomiroff": ["Princess Dragomiroff", "Princess"],
    "Ohlsson": ["Miss Ohlsson", "Greta Ohlsson"],
    "Constantine": ["Constantine", "Dr. Constantine"],
    "MacQueen": ["Hector MacQueen", "MacQueen"],
    "Mrs Armstrong": ["Mrs Armstrong"],
    "Antonio Foscarelli": ["Antonio", "Foscarelli", "Antonio Foscarelli"],
    "Miss Freebody": ["Miss Freebody"],
    "Helena Andrenyi": ["Helena", "Helena Elena", "Helena Goldenberg", "Helena Andrenyi"],
    "Elena": ["Elena", "Elena Maria", "Elena Andrenyi"],
    "Arden": ["Linda Arden", "Forest Arden Rosalind", "Arden"],
    "Samuel Edward Ratchett": ["Samuel Edward Ratchett", "Samuel Edward"],
    "Mary Debenham": ["Mary Debenham"]
}

normalized_entities = {synonym: key for key, synonyms in entity_synonyms.items() for synonym in synonyms}

def resolve_coreferences(sentences):
    resolved_sentences = []
    previous_entities = defaultdict(str)

    pronouns = ['he', 'she', 'his', 'her', 'him', 'it', 'its', 'they', 'them', 'their']

    for sentence in sentences:
        result = get_continuous_chunks(sentence)
        current_sentence = sentence
        replacements = []

        for entity in result:
            entity_text = normalized_entities.get(entity, entity)

            if entity_text.lower() in pronouns:
                entity_text = previous_entities[entity_text]

            previous_entities[entity_text] = entity_text

            replacements.append((entity, entity_text))

        for old, new in replacements:
            current_sentence = current_sentence.replace(old, new)

        resolved_sentences.append(current_sentence)

    return resolved_sentences

resolved_sentences = resolve_coreferences(filtered_sentences)

persons = []
locations = []
facilities = []
organizations=[]
labels = set()
ner_categories = defaultdict(int)

for sentence in resolved_sentences:
    result = get_continuous_chunks(sentence)
    for entity in result:
        entity_text = normalized_entities.get(entity, entity)

        if entity_text in entity_synonyms['PER']:
            persons.append(entity_text)
        elif entity_text in entity_synonyms['LOC']:
            locations.append(entity_text)
        elif entity_text in entity_synonyms['FAC']:
            facilities.append(entity_text)
        elif entity_text in entity_synonyms['ORG']:
            organizations.append(entity_text)
        labels.add(entity_text)

freq_dist = FreqDist(persons)
location_freq_dist = FreqDist(locations)
fac_freq = FreqDist(facilities)

print("Most frequent persons:")
for person, frequency in freq_dist.most_common(15):
    print(person, frequency)

print("\nMost frequent locations:")
for facility, frequency in location_freq_dist.most_common(15):
    print(facility, frequency)

"""As a conclusion, I've tried different way for corefernce resolution. However, my computer crashes when dealing with large data.
For future work, try to find the optiml way to handle this problem.

Now, let's shift the focus on findind locations
"""

! pip install locationtagger
import locationtagger

all_text = ' '.join(filtered_sentences)
entities = locationtagger.find_locations(text=all_text)

locations.extend(entities.cities)
locations.extend(entities.countries)
locations.extend(entities.regions)

freqq = FreqDist(entities.cities)
print("Most frequent locations:")
for location, frequency in freqq.most_common(10):
    print(location, frequency)

"""##Evaluation of NER: SOLUTION 1 with Nltk"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from collections import defaultdict
import random

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

normalized_entities = {synonym: key for key, synonyms in entity_synonyms.items() for synonym in synonyms}

def nltk_ner(sentence):
    words = word_tokenize(sentence)
    tagged_words = pos_tag(words)
    named_entities = ne_chunk(tagged_words)
    ner_results = defaultdict(list)
    for entity in named_entities:
        if hasattr(entity, 'label'):
            entity_name = ' '.join(c[0] for c in entity)
            normalized_name = normalized_entities.get(entity_name, entity_name)
            ner_results[entity.label()].append(normalized_name)
    return ner_results

sample_sentences = random.sample(filtered_sentences, min(50, len(filtered_sentences)))

sample_ner_results = [nltk_ner(sentence) for sentence in sample_sentences]
for i, (sentence, ner_result) in enumerate(zip(sample_sentences, sample_ner_results)):
    print(f"Sentence {i+1}: {sentence}")
    print("NER result:")
    for entity_type, entities in ner_result.items():
        print(f"  {entity_type}: {', '.join(entities)}")
    print()

from sklearn.metrics import precision_score, recall_score, f1_score

y_pred = ["None","None","GPE","None","None","None","GPE",  "None", "None","None","None",
          "None", "None", "PER","None"," None", " None"," None","PER","None","None",
          "None","None","PER","None","ORG","None", "None","None"," None"," None", "None","PER",
          "None"," None","None","None","None","GPE", "None","None","None","None",
          "PER","GPE","None","None","PER","PER","None"
]

y_true = ["PER","None","GPE","None","None","None","GPE","None", "PER","PER","None",
          "None", "None", "PER","PER","None", "None","None","PER","None","None",
          "None","None","PER","None","PER","None", "None","None",
          "None","None","None","PER","None","None","None","None","None",
          "GPE", "None","PER","None","None","GPE","GPE","None","None","PER","PER","None"
]

precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')

print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

"""For measuring Kappa distance, I have to write again the ratings1 and ratings2 between me and the other person.

and I calculate this:
"""

from sklearn.metrics import cohen_kappa_score


person_1 = ["PER","None","GPE","None","None","None","GPE", "None", "PER","PER","None","None",
             "None", "PER","PER","None", "None","None","PER","None","None","None","None","PER",
             "None","PER","None", "None","None","None","None", "None","PER","None","None","None",
             "None","None","GPE", "None","PER","None","None","GPE","GPE","None","None","PER","PER","None"
]

person_2 = ["PER","None","None","None","None","None","GPE","None","None","PER","None","None","None",
             "PER","None","None","None","None","PER","None","None","None","None","PER","None","ORG","None",
             "None","None","None","None","None","PER","None","None","None","None","None","GPE","None",
             "PER","None","None","GPE","None","None","None","PER","PER","None"
]
kappa = cohen_kappa_score(person_1, person_2)

print(f"Cohen's Kappa: {kappa}")

"""So, in this case, I have to discusss with the person and define a agreement on our label, in order to get a Kappa-value >= 0.8

I only need one agreement
We agreed on the sentence number: 3
"""

from sklearn.metrics import cohen_kappa_score

person_1 = [ "PER","None","None","None","None","None","GPE", "None", "PER","PER","None","None",
             "None", "PER","PER","None", "None","None","PER","None","None","None","None","PER",
             "None","PER","None", "None","None","None","None", "None","PER","None","None","None",
             "None","None","GPE", "None","PER","None","None","GPE","GPE","None","None","PER","PER","None"]

person_2 = ["PER","None","None","None","None","None","GPE","None","None","PER","None","None","None",
             "PER","None","None","None","None","PER","None","None","None","None","PER","None","ORG","None",
             "None","None","None","None","None","PER","None","None","None","None","None","GPE","None",
             "PER","None","None","GPE","None","None","None","PER","PER","None"

]

kappa = cohen_kappa_score(person_1, person_2)

print(f"Cohen's Kappa: {kappa}")

"""# Named Entity Recognition (NER) with BERT"""

!pip install accelerate

!pip install transformers

!pip install datasets

!pip install --upgrade transformers

!pip install --upgrade torch

"""##APPLYING BERT FOR NER: SOLUTION 2

"""

from transformers import pipeline
from collections import defaultdict
import matplotlib.pyplot as plt
from nltk.probability import FreqDist
import pandas as pd
import nltk
from nltk.chunk import ne_chunk
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from tqdm import tqdm

model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline("token-classification", model=model_checkpoint, aggregation_strategy="simple")

persons = []
locations = []
facilities = []
for sentence in tqdm(filtered_sentences, desc="Processing sentences"):
    result = token_classifier(sentence)
    for entity in result:
        entity_text = entity['word']
        entity_label = entity['entity_group']
        if entity_label == 'PER':
            persons.append(entity_text)
        elif entity_label == 'LOC':
            locations.append(entity_text)
        elif entity_label == 'FAC':
            facilities.append(entity_text)

freq_dist = FreqDist(persons)
location_freq_dist = FreqDist(locations)
fac_freq = FreqDist(facilities)

chunked_sentences = []
grammar = "NP: {<DT>?<JJ>*<NN>}"
cp = nltk.RegexpParser(grammar)
for sentence in tqdm(filtered_sentences, desc="Chunking sentences"):
    words = word_tokenize(sentence)
    tagged_words = pos_tag(words)
    chunked_sentence = cp.parse(tagged_words)
    chunked_sentences.append(chunked_sentence)

chunks = []
for chunked_sentence in tqdm(chunked_sentences, desc="Extracting chunks"):
    for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):
        chunks.append(' '.join([t[0] for t in subtree.leaves()]))

"""All the Frequencies of the People"""

print("Most frequent persons:")
for person, frequency in freq_dist.most_common(15):
    print(person, frequency)

print("Most frequent locations:")
for location, frequency in location_freq_dist.most_common(10):
    print(location, frequency)

"""##Adjusting the Results 2

So, now, to get more precise things; I can apply Coreference resolution
"""

!pip install allennlp

!pip install allennlp_models

"""#Results used:"""

from collections import defaultdict
from transformers import pipeline
import matplotlib.pyplot as plt
from nltk.probability import FreqDist
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

entity_synonyms = {
    "Poirot": ["Poirot", "M. Poirot", "Hercule Poirot", "M. Hercule Poirot", "Poiret", "POIROT", "Hercule Poirot —", "Monsieur Poirot", "##IROT","Po","HERCULE"],
    "M. Bouc": ["M. Bouc", "M. Bouc.", "Monsieur Bouc"],
    "Ratchett": ["Ratchett", "M. Ratchett", "Samuel Edward Ratchett", "Ratchett —", "Samuel Edward Ratchett —"],
    "Hubbard": ["Hubbard", "Mrs Hubbard", "Caroline Martha Hubbard"],
    "Arbuthnot": ["Arbuthnot"],
    "MacQueen": ["MacQueen", "M. MacQueen", "Hector MacQueen", "Hector Willard MacQueen"],
    "Armstrong": ["Armstrong", "Daisy Armstrong", "Sonia Armstrong", "Tommy Armstrong", "Selby Armstrong", "Toby Armstrong", "Armstrongs", "John Armstrong"],
    "Hardman": ["Hardman", "M. Hardman", "Cyrus Bethman Hardman", "Cyrus B. Hardman McNeil", "Cyrus Hardman", "Mr Hardman"],
    "Monsieur le Comte": ["Monsieur", "M. le", "Monsieur le Comte"],
    "Debenham": ["Debenham", "Mary Debenham", "Mary Hermione Debenham", "DEBEN", "Miss Debenham", "Mary —"],
    "Andrenyi": ["Andrenyi", "Elena Andrenyi", "Helena Andrenyi"],
    "Dragomiroff": ["Dragomiroff", "Madame la Princesse Dragomiroff", "Natalia Dragomiroff","Natalia","Madame la Princesse"],
    "Schmidt": ["Hildegarde Schmidt", "Fraulein Schmidt", "Schmidt"],
    "Michel": ["Michel", "Pierre Michel", "Michel —"],
    "Cassetti": ["Cassetti", "Cassetti —", "Cass"],
    "Arden": ["Linda Arden"],
    "Helena": ["Helena", "Elena", "Helena Goldenberg", "Elena Maria"],
    "Ohlsson": ["Greta Ohlsson", "Great Ohlsson", "Miss Ohlsson"],
    "Foscarelli": ["Antonio Foscarelli", "Foscarelli"],
    "Masterman": ["Masterman", "Edward Masterman", "Edward Henry Masterman"],
    "Harris": ["Harris", "M. Harris"],
    "Goldenberg": ["Helena Goldenberg", "Goldenberg"],
    "Daisy": ["Daisy", "Daisy —"],
    "Sonia": ["Sonia", "Sonia Armstrong"],
    "Miss Freebody": ["Miss Freebody", "Freebody"]
}

normalized_entities = {synonym: key for key, synonyms in entity_synonyms.items() for synonym in synonyms}
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline("token-classification", model=model_checkpoint, aggregation_strategy="simple")

def resolve_coreferences(sentences, token_classifier):
    resolved_sentences = []
    previous_entities = defaultdict(str)

    pronouns = ['he', 'she', 'his', 'her', 'him', 'it', 'its', 'they', 'them', 'their']

    for sentence in sentences:
        result = token_classifier(sentence)
        current_sentence = sentence
        replacements = []

        result = [{'word': normalized_entities.get(entity['word'], entity['word']), 'entity_group': entity['entity_group']} for entity in result]

        for entity in result:
            entity_text = entity['word']
            entity_label = entity['entity_group']

            if entity_label in ['PER', 'LOC', 'ORG', 'FAC']:
                if entity_text.lower() in pronouns:
                    entity_text = previous_entities[entity_label]

                previous_entities[entity_label] = entity_text

                replacements.append((entity['word'], entity_text))

        for old, new in replacements:
            current_sentence = current_sentence.replace(old, new)

        resolved_sentences.append(current_sentence)

    return resolved_sentences

resolved_sentences = resolve_coreferences(filtered_sentences, token_classifier)

persons = []
locations = []
facilities = []
organizations=[]
labels = set()
ner_categories = defaultdict(int)

for sentence in resolved_sentences:
    result = token_classifier(sentence)
    for entity in result:
        entity_text = entity['word']
        entity_label = entity['entity_group']

        entity_text = normalized_entities.get(entity_text, entity_text)

        if entity_label == 'PER':
            persons.append(entity_text)
        elif entity_label == 'LOC':
            locations.append(entity_text)
        elif entity_label == 'FAC':
            facilities.append(entity_text)
        elif entity_label == 'ORG':
            organizations.append(entity_text)
        labels.add(entity_label)

freq_dist = FreqDist(persons)
location_freq_dist = FreqDist(locations)
fac_freq = FreqDist(facilities)

"""This should work good for further analysis"""

from nltk.chunk import conlltags2tree, tree2conlltags
import json

def get_chunks(sentences):
    chunked_sentences = []

    for sentence in filtered_sentences:
        tokens = nltk.word_tokenize(sentence)
        tagged = nltk.pos_tag(tokens)
        named_ent = nltk.ne_chunk(tagged, binary=True)
        iob_tagged = tree2conlltags(named_ent)
        chunked_sentences.append(iob_tagged)

    return chunked_sentences

chunked_sentences = get_chunks(resolved_sentences)

data = {
    "persons": persons,
    "locations": locations,
    "facilities": facilities,
    "labels": list(labels),
    "ner_categories": dict(ner_categories),
    "freq_dist": dict(freq_dist),
    "location_freq_dist": dict(location_freq_dist),
    "fac_freq": dict(fac_freq),
    "chunked_sentences": chunked_sentences
}

with open('data.json', 'w') as f:
    json.dump(data, f)

print("Most frequent persons:")
for person, frequency in freq_dist.most_common(15):
    print(person, frequency)

print("Most frequent locations:")
for location, frequency in location_freq_dist.most_common(15):
    print(location, frequency)

"""Also in this case, as for Nltk, I've had problem in dealing with large text. Need further work, in order to find a way to don't have crashes of the computer

##Localization of the Places Mentioned
"""

!pip install geopy folium transformers nltk matplotlib torch

import folium
from geopy.geocoders import Nominatim

geolocator = Nominatim(user_agent="location_plotter")
map_center = [20.0, 0.0]
location_map = folium.Map(location=map_center, zoom_start=2)

top_locations = location_freq_dist.most_common(10)

for location, frequency in top_locations:
    try:
        if location.lower() == 'nile':
            nile_coords = (26.8206, 30.8025)
            folium.Marker(
                nile_coords,
                popup=f"{location}: {frequency} occurrences",
            ).add_to(location_map)
        else:
            geolocator = Nominatim(user_agent="location_plotter")
            loc = geolocator.geocode(location)
            if loc:
                folium.Marker(
                    [loc.latitude, loc.longitude],
                    popup=f"{location}: {frequency} occurrences",
                ).add_to(location_map)
    except Exception as e:
        print(f"Error geocoding {location}: {e}")

display(location_map)

"""##Evaluation of NER:SOLUTION 2"""

import random

sample_sentences = random.sample(filtered_sentences, 50)
sample_ner_results = [token_classifier(sentence) for sentence in sample_sentences]

for i, (sentence, result) in enumerate(zip(sample_sentences, sample_ner_results)):
    print(f"Sentence {i+1}: {sentence}")
    print(f"NER result: {result}")
    print()

"""Now, that I've extracted 50 sentences for NER, where each sentence and its corresponding NER result are printed out.

So here, I write my annotated data:
"""

from sklearn.metrics import precision_score, recall_score, f1_score

y_pred = ["ORG","None","PER","None","None","None",  "LOC", "None","None","None",
    "PER", "MISC", "LOC","MISC",
    "PER", "PER"," PER","MISC","None","None","None",
    "PER","PER","None","None","PER", "None","None",
    "LOC","PER", "None","None",
    "PER, LOC","None","None","None",
    "PER", "MISC","None","None","None",
    "None","None","None","None","None","None",
    "PER", "None","None","None","None","None","LOC",
    "LOC","None"
]

y_true = [
    "LOC","None","PER","None","None","None",  "LOC",
    "None","None","None","PER", "None", "PER","PER",
    "PER", "PER"," PER","None","None","None","None",
    "PER","PER","None","None","PER", "None","None",
    "LOC","PER", "None","None",
    "PER, LOC","None","None","None",
    "PER", "None","None","None","None",
    "None","None","None","None","None","None",
    "PER", "None","None","None","None","None","LOC",
    "LOC","None"
]

precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')

print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

"""For measuring Kappa distance, I have to write again the  ratings1 and ratings2 between me and the other person.

and I calculate this:
"""

from sklearn.metrics import cohen_kappa_score

person_1 = ["LOC","None","PER","None","None","None","LOC",
             "None","None","None","PER", "None", "PER","PER",
             "PER", "PER"," PER","None","None","None","None",
             "PER","PER","None","None","PER", "None","None",
             "LOC","PER", "None","None","PER, LOC","None",
             "None","None","PER", "None","None","None","None",
             "None","None","None","None","None","None",
             "PER", "None","None","None","None","None","LOC",
             "LOC","None"]


person_2 = ["MISC","None","PER","None","None","None","LOC",
             "None","None","None","PER", "MISC", "LOC","MISC",
             "PER", "PER"," PER","MISC","None","None","None",
             "PER","PER","None","None","PER", "None","None",
             "LOC","PER", "None","None","PER, LOC","None",
             "None","None","PER", "MISC","None","None","None",
             "None","None","None","None","None","None",
             "PER", "None","None","None","None","None","MISC",
             "MISC","None"]

kappa = cohen_kappa_score(person_1, person_2)

print(f"Cohen's Kappa: {kappa}")

"""In this case, we also have to agreed wich one to change and to use, in order to have a Kappa distance<=0.8

So, I have changed the sentences: 1, 48,49
"""

from sklearn.metrics import cohen_kappa_score

person_1 = ["MISC","None","PER","None","None","None","LOC",
             "None","None","None","PER", "None", "PER","PER",
             "PER", "PER"," PER","None","None","None","None",
             "PER","PER","None","None","PER", "None","None",
             "LOC","PER", "None","None","PER, LOC","None",
             "None","None","PER", "None","None","None","None",
             "None","None","None","None","None","None",
             "PER", "None","None","None","None","None","MISC",
             "MISC","None"]


person_2 = ["MISC","None","PER","None","None","None","LOC",
             "None","None","None","PER", "MISC", "LOC","MISC",
             "PER", "PER"," PER","MISC","None","None","None",
             "PER","PER","None","None","PER", "None","None",
             "LOC","PER", "None","None","PER, LOC","None",
             "None","None","PER", "MISC","None","None","None",
             "None","None","None","None","None","None",
             "PER", "None","None","None","None","None","MISC",
             "MISC","None"]

kappa = cohen_kappa_score(person_1, person_2)

print(f"Cohen's Kappa: {kappa}")

"""#Sentiment Analysis


Before starting with sentiment analysis, I want to check if the model is capable to handle negations.

It should be, because the pipeline that I will use is pre-trained and already fine-tuned.

I do anyways a check: for this case, I've taken 10 random negative sentences, and checked if the model is capable to detect them.
"""

negative_sentences = [
    "The project hasn't started yet.",
    "You shouldn't leave your bag unattended.",
    "She doesn't like spicy food.",
    "He never goes to the gym.",
    "They won't agree to those terms.",
    "I haven't read that book yet.",
    "The dog isn't in the yard.",
    "We don't have enough time to finish this.",
    "You can't park here.",
    "The restaurant doesn't serve breakfast."
]

from transformers import pipeline
import matplotlib.pyplot as plt
from tqdm import tqdm

sentiment_classifier = pipeline("sentiment-analysis")

sentiment_labels = []
sentiment_scores = []

for sentence in tqdm(negative_sentences, desc="Processing sentences"):
    result = sentiment_classifier(sentence)
    label = result[0]['label']
    score = result[0]['score']
    sentiment_labels.append(label)
    sentiment_scores.append(score)

import numpy as np
import matplotlib.pyplot as plt

numeric_labels = [1 if label == 'POSITIVE' else 0 for label in sentiment_labels]
positive_labels = [label for label in numeric_labels if label == 1]
negative_labels = [label for label in numeric_labels if label == 0]

plt.hist(positive_labels, bins=np.arange(-0.5, 2, 1), color='green', alpha=0.7, rwidth=0.8, label='Positive')
plt.hist(negative_labels, bins=np.arange(-0.5, 2, 1), color='red', alpha=0.7, rwidth=0.8, label='Negative')
plt.title('Histogram of Sentiment Labels')
plt.xlabel('Label (0:Negative, 1:Positive)')
plt.ylabel('Frequency')
plt.legend()

plt.tight_layout()
plt.show()

"""From this result, I can suppose that the model is capable to handle negations, so I can continue to do sentiment analysis for all the text.

"""

from transformers import pipeline
import matplotlib.pyplot as plt
from tqdm import tqdm

sentiment_classifier = pipeline("sentiment-analysis")

sentiment_labels = []
sentiment_scores = []

for sentence in tqdm(resolved_sentences, desc="Processing sentences"):
    result = sentiment_classifier(sentence)
    label = result[0]['label']
    score = result[0]['score']
    sentiment_labels.append(label)
    sentiment_scores.append(score)

import numpy as np
import matplotlib.pyplot as plt

numeric_labels = [1 if label == 'POSITIVE' else 0 for label in sentiment_labels]
positive_labels = [label for label in numeric_labels if label == 1]
negative_labels = [label for label in numeric_labels if label == 0]

plt.hist(positive_labels, bins=np.arange(-0.5, 2, 1), color='green', alpha=0.7, rwidth=0.8, label='Positive')
plt.hist(negative_labels, bins=np.arange(-0.5, 2, 1), color='red', alpha=0.7, rwidth=0.8, label='Negative')
plt.title('Histogram of Sentiment Labels')
plt.xlabel('Label (0:Negative, 1:Positive)')
plt.ylabel('Frequency')
plt.legend()

plt.tight_layout()
plt.show()

import pandas as pd

positive_scores_series = pd.Series([score for label, score in zip(sentiment_labels, sentiment_scores) if label == 'POSITIVE'])
negative_scores_series = pd.Series([score for label, score in zip(sentiment_labels, sentiment_scores) if label == 'NEGATIVE'])

positive_scores_ma = positive_scores_series.rolling(window=50).mean()
negative_scores_ma = negative_scores_series.rolling(window=50).mean()

plt.plot(positive_scores_series.index, positive_scores_ma, 'g', label='Positive')
plt.plot(negative_scores_series.index, negative_scores_ma, 'r', label='Negative')
plt.legend(loc='upper right')
plt.xlabel('Sentence Order')
plt.ylabel('Sentiment Score')
plt.title('Sentiment Over Time (Moving Average)')
plt.show()

"""We can see how the sentiment of the last part of the book is very negative.



In this part I'm trying to do sentiment analysis for each character
"""

print("Most frequent persons:")
for person, frequency in freq_dist.most_common(15):
  print(person,frequency)

from collections import defaultdict

character_sentiments = defaultdict(list)

characters = ["Poirot","M. Bouc","Ratchett","Hubbard","MacQueen","Armstrong",
"Arbuthnot","Debenham","Hardman","Monsieur le Comte","Constantine","Michel","Andrenyi","Dragomiroff","Schmidt" ]

for character in characters:
    character_sentiments[character] = [0] * len(filtered_sentences)

for i, sentence in enumerate(tqdm(resolved_sentences, desc="Processing sentences")):
    for character in characters:
        if character in sentence:
            result = sentiment_classifier(sentence)
            score = result[0]['score']
            character_sentiments[character][i] = score

import numpy as np
import matplotlib.pyplot as plt

def moving_average(a, n=3) :
    ret = np.cumsum(a, dtype=float)
    ret[n:] = ret[n:] - ret[:-n]
    return ret[n - 1:] / n

for character, sentiments in character_sentiments.items():
    sentiments_smooth = moving_average(np.array(sentiments), n=50)
    plt.figure()
    plt.plot(sentiments_smooth, label=character)
    plt.xlabel('Progression of the Book')
    plt.ylabel('Sentiment Score')
    plt.title(f'Sentiment Analysis of {character} Over the Book')
    plt.legend()

    plt.show()

"""This problem occurs because some characters are less frequent, and by consequence are not present constantly over time in the book.

Future work can be adressed in order to overcome this problem

#Emotion Analysis
"""

from transformers import pipeline
classifier = pipeline("text-classification",model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)

emotion_predictions = [classifier(sentence) for sentence in resolved_sentences]

for i, (sentence, prediction) in enumerate(zip(resolved_sentences, emotion_predictions)):
    print(f"Sentence {i+1}: {sentence}")
    print(f"Emotion prediction: {prediction}")
    print()

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'Sentence': filtered_sentences,
    'Emotion': [pred[0][0]['label'] for pred in emotion_predictions],
    'Score': [pred[0][0]['score'] for pred in emotion_predictions],
    'Progression': np.linspace(0, 1, len(filtered_sentences))
})

plt.figure(figsize=(10, 6))
for emotion in df['Emotion'].unique():
    subset_df = df[df['Emotion'] == emotion]
    plt.plot(subset_df['Progression'], subset_df['Score'], label=emotion)

plt.xlabel('Progression in the Book')
plt.ylabel('Emotion Score')
plt.title('Emotion Score Over the Progression of the Book')
plt.legend()
plt.show()

"""Also in this case, the plot is really cahotic.
I decide to plot the moving average
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

data = []
for i, pred in enumerate(emotion_predictions):
    for emotion in pred[0]:
        data.append({
            'Sentence': filtered_sentences[i],
            'Emotion': emotion['label'],
            'Score': emotion['score'],
            'Progression': i / len(filtered_sentences)
        })

df = pd.DataFrame(data)

df['Moving_Average'] = df.groupby('Emotion')['Score'].rolling(window=50).mean().reset_index(0, drop=True)

plt.figure(figsize=(10, 6))
for emotion in df['Emotion'].unique():
    subset_df = df[df['Emotion'] == emotion]
    plt.plot(subset_df['Progression'], subset_df['Moving_Average'], label=emotion)

plt.xlabel('Progression in the Book')
plt.ylabel('Emotion Score')
plt.title('Moving Average of Emotion Score Over the Progression of the Book')
plt.legend()
plt.show()

"""Here, another type of visalization"""

from wordcloud import WordCloud
emotion_string = ' '.join([pred_dict['label'] for prediction in emotion_predictions for pred_dict in prediction[0]])
wordcloud = WordCloud(width=800, height=400, random_state=21, max_words=1000,max_font_size=110, colormap='twilight').generate(emotion_string)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""#Character-Words Association

I complete this task in order to discover associations and for further create a Knowledge Graph
"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

top_characters = ["Poirot","M. Bouc","Ratchett","Hubbard","MacQueen","Armstrong",
"Arbuthnot","Debenham","Hardman","Monsieur le Comte","Michel","Andrenyi","Dragomiroff","Schmidt" ]

character_freq_dist = {}
for character in top_characters:
    character_freq_dist[character] = nltk.FreqDist()

for sentence in resolved_sentences:
    for character in top_characters:
        if any(synonym.lower() in sentence.lower() for synonym in entity_synonyms[character]):
            words = nltk.word_tokenize(sentence)
            words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word not in stop_words]
            character_freq_dist[character].update(words)

character_word_frequencies = {}
for character, freq_dist in character_freq_dist.items():
    sorted_words = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)
    character_word_frequencies[character] = sorted_words

for character, word_frequencies in character_word_frequencies.items():
    print(f"Top words associated with {character}:")
    for word, frequency in word_frequencies[:15]:
        print(word, frequency)
    print()

"""Another possible way is this one:

This way with occurence matrix is possible:
"""

from collections import defaultdict

co_occurrence_matrix = defaultdict(lambda: defaultdict(int))
for sentence in resolved_sentences:
    for character in top_characters:
        if any(synonym.lower() in sentence.lower() for synonym in entity_synonyms[character]):
            words = nltk.word_tokenize(sentence)
            words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word not in stop_words]
            for i in range(len(words) - 1):
                for j in range(i + 1, len(words)):
                    word1, word2 = sorted([words[i], words[j]])
                    co_occurrence_matrix[word1][word2] += 1

for character in top_characters:
    print(f"Top co-occurring words associated with {character}:")
    for word1 in co_occurrence_matrix:
        for word2, freq in sorted(co_occurrence_matrix[word1].items(), key=lambda x: x[1], reverse=True):
            print(f"{word1} & {word2}: {freq}")
    print()

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

co_occurrence_matrix = defaultdict(lambda: defaultdict(int))

for sentence in resolved_sentences:
    for character in top_characters:
        if any(synonym.lower() in sentence.lower() for synonym in entity_synonyms[character]):
            words = nltk.word_tokenize(sentence)
            words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word not in stop_words]
            for i in range(len(words) - 1):
                for j in range(i + 1, len(words)):
                    word1, word2 = sorted([words[i], words[j]])
                    co_occurrence_matrix[word1][word2] += 1

"""#Create a Knowledge Graph"""

G = nx.Graph()

for character in top_characters:
    G.add_node(character, type='character')

for word1 in co_occurrence_matrix:
    for word2, freq in co_occurrence_matrix[word1].items():
        if freq > 1:
            G.add_node(word1, type='word')
            G.add_node(word2, type='word')
            G.add_edge(word1, word2, weight=freq)

pos = nx.spring_layout(G, k=0.5, iterations=50)
plt.figure(figsize=(12, 12))

character_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'character']
word_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'word']
nx.draw_networkx_nodes(G, pos, nodelist=character_nodes, node_color='red', node_size=500, alpha=0.8, label='Characters')
nx.draw_networkx_nodes(G, pos, nodelist=word_nodes, node_color='blue', node_size=300, alpha=0.6, label='Words')

edges = G.edges(data=True)
nx.draw_networkx_edges(G, pos, edgelist=edges, width=[d['weight'] for (u, v, d) in edges], alpha=0.5)

nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')

plt.title('Knowledge Graph of Character-Word Associations')
plt.legend()
plt.show()

"""NOW, I CAN VISUALIZE ONLY A SMALL PART OF THE GRAPH FOR A COUPLE OF NODES:"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import defaultdict, Counter
import networkx as nx
import matplotlib.pyplot as plt


top_characters = ["MacQueen", "Ratchett", "Armstrong"]

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def get_character_word_counts(character, sentences):
    word_counts = Counter()
    for sentence in sentences:
        if any(synonym.lower() in sentence.lower() for synonym in entity_synonyms[character]):
            words = nltk.word_tokenize(sentence)
            words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word not in stop_words]
            word_counts.update(words)
    return word_counts

character_top_words = {}
for character in top_characters:
    word_counts = get_character_word_counts(character, resolved_sentences)
    top_words = word_counts.most_common(10)
    character_top_words[character] = top_words

co_occurrence_matrix = defaultdict(lambda: defaultdict(int))

for sentence in resolved_sentences:
    words = nltk.word_tokenize(sentence)
    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word not in stop_words]
    for i in range(len(words) - 1):
        for j in range(i + 1, len(words)):
            word1, word2 = sorted([words[i], words[j]])
            co_occurrence_matrix[word1][word2] += 1

G = nx.Graph()

for character in top_characters:
    G.add_node(character, type='character')

for character, top_words in character_top_words.items():
    for word, freq in top_words:
        if word not in G:
            G.add_node(word, type='word')
        G.add_edge(character, word, weight=co_occurrence_matrix[character].get(word, 1))

pos = nx.spring_layout(G, k=0.5, iterations=50)
plt.figure(figsize=(12, 12))

character_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'character']
word_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'word']
nx.draw_networkx_nodes(G, pos, nodelist=character_nodes, node_color='red', node_size=500, alpha=0.8, label='Characters')
nx.draw_networkx_nodes(G, pos, nodelist=word_nodes, node_color='blue', node_size=300, alpha=0.6, label='Words')

edges = G.edges(data=True)
nx.draw_networkx_edges(G, pos, edgelist=edges, width=[d['weight'] for (u, v, d) in edges], alpha=0.5)


nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')

plt.title('Knowledge Graph of Character-Word Associations')
plt.legend()
plt.show()

"""#Topic Modelling"""

from nltk.corpus import stopwords
from gensim import corpora, models

stop_words = set(stopwords.words('english'))

def prepare_corpus(lemmatized_tokens):
    lemmatized_tokens = [word for word in lemmatized_tokens if word not in stop_words]
    dictionary = corpora.Dictionary([lemmatized_tokens])
    doc_term_matrix = [dictionary.doc2bow(lemmatized_tokens)]
    return dictionary, doc_term_matrix

number_of_topics = 4

dictionary, doc_term_matrix = prepare_corpus(lemmatized_tokens)
lsa_topic_model = models.LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)

for topic_num, words in lsa_topic_model.show_topics():
    print('Representative words of topic {}: {}.'.format(topic_num, words))

lda_model = models.LdaModel(doc_term_matrix, num_topics=4, id2word=dictionary, passes=30, iterations=400)
topics = lda_model.print_topics(num_topics=4, num_words=10)
for topic in topics:
    print(topic)

from gensim.models import CoherenceModel

coherencemodel = CoherenceModel(model=lsa_topic_model, texts=[lemmatized_tokens], dictionary=dictionary, coherence='c_npmi')
coherence = coherencemodel.get_coherence()

print('Coherence Score: ', coherence)

"""I do not have good result

##LDA
"""

!pip install pyLDAvis

from gensim.models import ldamodel
import pyLDAvis
from pyLDAvis import gensim_models

lda_topic_model = ldamodel.LdaModel(
    corpus=doc_term_matrix,
    id2word=dictionary,
    num_topics=20,
    random_state=100,
    passes=10,
    alpha='symmetric',
    eta='symmetric',
    decay=0.5,
    offset=1.0
)

for topic_num, words in lda_topic_model.show_topics():
    print('Representative words of topic {}: {}.'.format(topic_num, words))

coherencemodel = CoherenceModel(model=lda_topic_model, texts=[lemmatized_tokens], dictionary=dictionary, coherence='c_uci')
coherence = coherencemodel.get_coherence()

print('Coherence Score: ', coherence)

pyLDAvis.enable_notebook()
visualization = gensim_models.prepare(
    lda_topic_model,
    doc_term_matrix,
    dictionary,
    mds = "mmds",
    R = 30)

visualization

"""##NMF

"""

from gensim.models import Nmf

nmf_topic_model = Nmf(
    corpus = doc_term_matrix,
    id2word = dictionary,
    num_topics = 8,
    random_state = 100,
    passes = 10,
    )

for topic_num, words in nmf_topic_model.show_topics():
    print('Representative words of topic {}: {}.'.format(topic_num, words))

coherencemodel = CoherenceModel(model=nmf_topic_model, texts=[lemmatized_tokens], dictionary=dictionary, coherence='c_v')
coherencemodel.get_coherence()

"""The results do not convince

##BERTopic
"""

!pip install bertopic

from bertopic import BERTopic

topic_model = BERTopic()
topics, probs = topic_model.fit_transform(resolved_sentences)

topic_model.get_topics()

topic_model.get_topic_info()

"""This visualization applies UMAP to show a 2D visualization of topics and their distance. The size of the bubble reflects the number of documents in the topic. Similar documents should be close to each other."""

topic_model.visualize_topics()

topic_model.visualize_documents(resolved_sentences, hide_annotations=True)

"""Hierchically reducing the number of topics based on their similarity. It shows how different clusters relate to each other, and can be used to find the number of topics or to find similar topics and merge them."""

from sklearn.datasets import fetch_20newsgroups

from scipy.cluster import hierarchy as sch
linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)
hierarchical_topics = topic_model.hierarchical_topics(resolved_sentences, linkage_function=linkage_function)

topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)

"""If you hover over the black circles, you will see the topic representation at that level of the hierarchy. These representations help you understand the effect of merging certain topics. Some might be logical to merge whilst others might not. Moreover, we can now see which sub-topics can be found within certain larger themes.

Although this gives a nice overview of the potential hierarchy, hovering over all black circles can be tiresome. Instead, we can use topic_model.get_topic_tree to create a text-based representation of this hierarchy. Although the general structure is more difficult to view, we can see better which topics could be logically merged:
"""

tree = topic_model.get_topic_tree(hierarchical_topics)
print(tree)

import datetime

list_of_tuples = [(sentence, datetime.datetime.now()) for sentence in resolved_sentences]
for item in list_of_tuples:
    print(item)

list_of_tuples = [(sentence, datetime.datetime.now()) for sentence in resolved_sentences]
df = pd.DataFrame(list_of_tuples, columns=['text', 'date'])

timestamps = df.date.to_list()
sentences = df.text.to_list()

topic_model = BERTopic(verbose=True)
topics, probs = topic_model.fit_transform(sentences)

topics_over_time = topic_model.topics_over_time(sentences, timestamps, nr_bins=20)

topics_over_time = topic_model.topics_over_time(sentences, timestamps,
                                                global_tuning=True, evolution_tuning=True, nr_bins=20)

topics_over_time = topic_model.topics_over_time(sentences, timestamps, nr_bins=20)

topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)

"""Let's try reducing the number of topics"""

from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer

vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
topic_model = BERTopic(vectorizer_model=vectorizer_model)

number_of_topics = 30
topic_model = BERTopic(vectorizer_model=vectorizer_model, nr_topics=number_of_topics)
topics, probs = topic_model.fit_transform(resolved_sentences)

topic_model.get_topic_info()

topic_model.visualize_topics()

topics_to_merge=[(1,13), (23,28)]

topic_model.merge_topics(resolved_sentences,topics_to_merge)


topic_model.visualize_topics()

df = pd.DataFrame(list_of_tuples, columns=['text', 'date'])

timestamps = df.date.to_list()
sentences = df.text.to_list()

number_of_topics = 30
topic_model = BERTopic(vectorizer_model=vectorizer_model, nr_topics=number_of_topics,verbose=True)

topics, probs = topic_model.fit_transform(sentences)

topics_over_time = topic_model.topics_over_time(sentences, timestamps,
                                                global_tuning=True, evolution_tuning=True, nr_bins=20)

topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)

"""#Co-Occurence Data:

Let's try to discover some insights:
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

characters = ["Poirot","M. Bouc","Ratchett","Hubbard","MacQueen","Armstrong",
"Arbuthnot","Debenham","Hardman","Monsieur le Comte","Constantine","Michel","Andrenyi","Dragomiroff","Schmidt" ]

persons_list = list(set(characters))

co_occurrence_matrix = pd.DataFrame(0, index=persons_list, columns=persons_list)

for sentence in resolved_sentences:
    mentioned_persons = [person for person in persons_list if person in sentence]
    for i, person1 in enumerate(mentioned_persons):
        for person2 in mentioned_persons[i+1:]:
            co_occurrence_matrix.loc[person1, person2] += 1
            co_occurrence_matrix.loc[person2, person1] += 1

plt.figure(figsize=(12, 10))
sns.heatmap(co_occurrence_matrix, annot=True, cmap="YlGnBu")
plt.title('Co-occurrence Heatmap of Characters')
plt.show()

import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from collections import defaultdict



characters = ["Poirot", "Constantine", "Michel", "Andrenyi", "Dragomiroff", "Schmidt"]
clue_keywords = [
    "footprint", "cigarette", "glove", "letter", "knife", "gun", "blood", "hair",
    "note", "jewel", "fingerprint", "key", "map", "document", "shoe", "watch",
    "ring", "phone", "wallet", "hat", "scarf", "belt", "button", "handkerchief",
    "eyeglasses", "photo", "diary", "pen", "cup", "candle", "tape", "book",
    "vase", "bottle", "umbrella", "earring", "necklace"
]


events = []
clues = defaultdict(list)

for sentence in filtered_sentences:
    for character in characters:
        if character in sentence:
            events.append((character, sentence))
            for clue in clue_keywords:
                if clue in sentence:
                    clues[clue].append((character, sentence))
                    break

events_df = pd.DataFrame(events, columns=['Character', 'Event'])
clues_df = pd.DataFrame([(clue, character, sentence) for clue, entries in clues.items() for character, sentence in entries], columns=['Clue', 'Character', 'Sentence'])

plt.figure(figsize=(12, 7))
sns.countplot(data=events_df, y='Character', order=events_df['Character'].value_counts().index)
plt.title('Number of Events per Character')
plt.xlabel('Count')
plt.ylabel('Character')
plt.show()

top_clues = clues_df['Clue'].value_counts().nlargest(15)
plt.figure(figsize=(12, 7))
sns.barplot(y=top_clues.index, x=top_clues.values, palette='viridis')
plt.title('Top 15 Clues')
plt.xlabel('Frequency')
plt.ylabel('Clue')
plt.show()

G = nx.Graph()

for character in characters:
    G.add_node(character, type='character')

for clue in clues:
    G.add_node(clue, type='clue')

for clue, entries in clues.items():
    for character, _ in entries:
        G.add_edge(character, clue)

plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G, k=0.5, iterations=50)

character_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'character']
clue_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'clue']
nx.draw_networkx_nodes(G, pos, nodelist=character_nodes, node_color='red', node_size=500, alpha=0.8, label='Characters')
nx.draw_networkx_nodes(G, pos, nodelist=clue_nodes, node_color='blue', node_size=300, alpha=0.6, label='Clues')

nx.draw_networkx_edges(G, pos, alpha=0.5)
nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')

plt.title('Knowledge Graph of Character-Word Associations')
plt.legend()
plt.show()

"""List of clue words and some characters:"""

import networkx as nx
import matplotlib.pyplot as plt
from collections import defaultdict
import pandas as pd



characters = ["Poirot", "Constantine", "Michel", "Andrenyi", "Dragomiroff", "Schmidt"]
clue_keywords = [
    "footprint", "cigarette", "glove", "letter", "knife", "gun", "blood", "hair",
    "note", "jewel", "fingerprint", "key", "map", "document", "shoe", "watch",
    "ring", "phone", "wallet", "hat", "scarf", "belt", "button", "handkerchief",
    "eyeglasses", "photo", "diary", "pen", "cup", "candle", "tape", "book",
    "vase", "bottle", "umbrella", "earring", "necklace"
]

co_occurrence_matrix = defaultdict(lambda: defaultdict(int))

for sentence in resolved_sentences:
    for character in characters:
        if character in sentence:
            for clue in clue_keywords:
                if clue in sentence:
                    co_occurrence_matrix[character][clue] += 1

co_occurrence_df = pd.DataFrame(co_occurrence_matrix).fillna(0)

G = nx.Graph()

for character in co_occurrence_df.index:
    G.add_node(character, type='character')

for clue in co_occurrence_df.columns:
    G.add_node(clue, type='clue')

for character in co_occurrence_df.index:
    for clue in co_occurrence_df.columns:
        weight = co_occurrence_df.loc[character, clue]
        if weight > 0:
            G.add_edge(character, clue, weight=weight)

degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
eigenvector_centrality = nx.eigenvector_centrality(G)

print("Degree Centrality:", degree_centrality)
print("Betweenness Centrality:", betweenness_centrality)
print("Eigenvector Centrality:", eigenvector_centrality)

plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G, k=0.5, iterations=50)

character_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'character']
clue_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'clue']
nx.draw_networkx_nodes(G, pos, nodelist=character_nodes, node_color='red', node_size=500, alpha=0.8, label='Clues')
nx.draw_networkx_nodes(G, pos, nodelist=clue_nodes, node_color='blue', node_size=300, alpha=0.6, label='Characters')

nx.draw_networkx_edges(G, pos, alpha=0.5)
nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')

plt.title('Knowledge Graph of Character-Clue Associations')
plt.legend()
plt.show()

"""Visualize centrality metrics"""

plt.figure(figsize=(12, 6))
degree_centrality_series = pd.Series(degree_centrality).sort_values(ascending=False)
degree_centrality_series.plot(kind='bar')
plt.title('Degree Centrality of Nodes')
plt.xlabel('Node')
plt.ylabel('Degree Centrality')
plt.show()

plt.figure(figsize=(12, 6))
betweenness_centrality_series = pd.Series(betweenness_centrality).sort_values(ascending=False)
betweenness_centrality_series.plot(kind='bar')
plt.title('Betweenness Centrality of Nodes')
plt.xlabel('Node')
plt.ylabel('Betweenness Centrality')
plt.show()

plt.figure(figsize=(12, 6))
eigenvector_centrality_series = pd.Series(eigenvector_centrality).sort_values(ascending=False)
eigenvector_centrality_series.plot(kind='bar')
plt.title('Eigenvector Centrality of Nodes')
plt.xlabel('Node')
plt.ylabel('Eigenvector Centrality')
plt.show()

"""I get the community"""

from networkx.algorithms.community import greedy_modularity_communities

communities = greedy_modularity_communities(G)
for i, community in enumerate(communities):
    print(f"Community {i+1}: {sorted(community)}")

"""For further work, create a model that extract clues automatically;

Because, in this case, I am extracting clues that I only suppose, so I can not know if they are correct

Let's analyse the most frequent locations mentioned:
"""

location_contexts = {location: [] for location in location_freq_dist.keys()}
for sentence in resolved_sentences:
    for location in location_contexts:
        if location in sentence:
            location_contexts[location].append(sentence)

for location, contexts in location_contexts.items():
    print(f"Context for {location}:")
    for context in contexts:
        print(f" - {context}")

key_events = ["train", "departure", "arrival", "stop", "incident"]

event_contexts = {event: [] for event in key_events}
for sentence in resolved_sentences:
    for event in key_events:
        if event in sentence.lower():
            event_contexts[event].append(sentence)

for event, contexts in event_contexts.items():
    print(f"Context for {event}:")
    for context in contexts:
        print(f" - {context}")

import matplotlib.pyplot as plt
from wordcloud import WordCloud


plt.figure(figsize=(12, 6))
for event, contexts in event_contexts.items():
    plt.plot_date([resolved_sentences.index(context) for context in contexts], [event]*len(contexts), label=event)
plt.title('Timeline of Key Events')
plt.xlabel('Sentence Index')
plt.ylabel('Event')
plt.legend()
plt.grid(True)
plt.show()

"""Study each of the charachters with the occurence with one"""

characters = ["Poirot","M. Bouc","Ratchett","Hubbard","MacQueen","Armstrong",
"Arbuthnot","Debenham","Hardman","Monsieur le Comte","Michel","Andrenyi","Dragomiroff","Schmidt"]

murder_elements = ["letter", "blood", "knife", "suspect","lie","escape"]

co_occurrence_matrix_characters = pd.DataFrame(0, index=characters, columns=characters)
co_occurrence_matrix_elements = pd.DataFrame(0, index=characters, columns=murder_elements)

for sentence in filtered_sentences:
    mentioned_characters = [character for character in characters if character in sentence]
    mentioned_elements = [element for element in murder_elements if element in sentence]

    for i, char1 in enumerate(mentioned_characters):
        for char2 in mentioned_characters[i+1:]:
            co_occurrence_matrix_characters.loc[char1, char2] += 1
            co_occurrence_matrix_characters.loc[char2, char1] += 1

    for char in mentioned_characters:
        for element in mentioned_elements:
            co_occurrence_matrix_elements.loc[char, element] += 1

plt.figure(figsize=(12, 10))
sns.heatmap(co_occurrence_matrix_elements, annot=True, cmap="YlGnBu")
plt.title('Co-occurrence Heatmap of Characters with Murder Elements')
plt.show()











